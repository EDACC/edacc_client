\documentclass{article}
\usepackage[utf8]{inputenc}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{.75in}

\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{.75in}


\title{EDACC Computation Client - User Guide}
\author{Daniel Diepold and Simon Gerber}
\date{\today}


\begin{document}
\maketitle
\newpage

\section{Introduction}
EDACC (Experiment Design and Analysis for Computer Clusters) is a software platform for the experimental analysis of algorithms.
It consists of multiple components: a MySQL database, a Java GUI application, a web frontend and the computation client (also simply called client in this document).
The GUI application can be used to create experiments that consist of a set of solvers that should run on a set of problem instances multiple times each.
This results in computation tasks that are also called jobs in EDACC. All instance and solver binaries as well as the experiments
and their results are stored in the database. To compute the jobs the computation client is used. Usually there have to be a lot of jobs computed to evaluate some experiments
and since they are independent from each other, this task can be parallelized across many CPU cores. The computation client can be started on arbitrarily many
machines and will manage the available CPUs and fetch and compute jobs from the created experiments. It connects to the central database and downloads all required ressources
such as instances and solver binaries and writes back the results to the database.

\section{System requirements}
The client is written in C/C++ and should be able to run on most Linux distributions or POSIX compliant operating systems where a MySQL C connector library is available.
Because the central storage location for all required computational ressources, experiment metadata and results is a MySQL database, the client has to be able
to establish a connection to the machine that hosts the database. This means that the machines where the client runs on have to be able to establish a TCP/IP connection
to the database machine. The client was mainly tested on the bwGRID\footnote{{http://www.bw-grid.de/}}, a distributed computer cluster that consists of several hundred
nodes at several physical locations at universities of Baden-WÃ¼rttemberg, Germany. Even though the machine hardware is homogenous, the network topology of bwGRID is not.
In cases where direct network access from the computation nodes back to the database server is not possible it is usually possible to tunnel a connection
over the cluster's login node back to the database via SSH.

Other than that, the client has to be able to write temporary files to some location on the filesystem. This can be configured if it differs from the client binary location.

Because the client will download missing solver binaries and instances and upload results it also needs a reasonably fast network connection to the database. Shared filesystems
can considerably reduce the required bandwidth since every file is only downloaded once. As alternative you can pre-package the required files and put them into the correct folders
before starting the client. However, if you modify experiments while the client is running it will still download missing files.

\newpage
\section{Usage}
\subsection{Configuration}
After designing experiments and creating jobs using the GUI application the client has to be configured. This is done by some command line arguments and a simple configuration file (called ''config'').
In the configuration file you have to specify the database connection details and which hardware the client runs on. This is done by configuring so called "grid queues" in the GUI application.
They contain some basic information about the computation hardware such as number of CPUs per machine. The client will then use this information to run as many parallel jobs as
the grid queue information allows it on each machine where it is launched. Here is a sample configuration file:
\begin{verbatim}
host = database.host.foo.com
username = dbusername
password = dbpassword
database = dbname
gridqueue = 3
verifier = ./verifiers/SAT
\end{verbatim}
Note that the gridqueue is simply the ID of the grid queue. Another (optional) configuration option is the verifier line. It tells the client if it should run a program on the output that a
solver generated. For example, if your experiments consist of attempting to solve boolean propositional logic formulas you can use a SAT verifier that tests if the solution given by a solver
is actually correct. The verifier configuration option is simply a path to an executable that the client calls. See section ???? for more information on verifiers.


Beside the configuration file there are several command line options the client accepts, please also see ''./client --help'':
\begin{verbatim}
-v <verbosity>: integer value between 0 and 4 (from lowest to highest verbosity)
-l: if flag is set, the log output is written to a
	file instead of stdout.
-w <wait for jobs time (s)>: how long the client should
wait for jobs after it didn't get any new jobs before exiting.
-i <handle workers interval ms>: how long the client should
wait after handling workers and before looking for a new job.
-k: whether to keep the solver and watcher output files or
to delete them after uploading to the DB.
\end{verbatim}

Verbosity controls the amount of log output the client generates. A value of 4 is only useful for debugging purposes, a value of 0 will make the client log important messages and all errors.

If the ''l'' flag is set, log output goes to a file whose name includes the hostname and IP address the client runs on to avoid name clashes in shared filesystems typically found in computer clusters.
Otherwise log output goes to standard output.

With the ''-w'' option you can tell the client how long to wait before exiting after it didn't start any jobs. This can be useful to keep the client running and ready to process new jobs
while you evaluate preliminary results and add new jobs or whole experiments. The wait option is also used to determine how long attempts should be made to reconnect to the database after
connection losses. The default value is 10 seconds.

The ''-i'' option controls how long the client should wait between its main processing loop iterations. If this value is low, it will more often look for new jobs when there are unused CPUs.
For maximum job throughput this value should be lower than the average job processing time but lower values will also put more strain on the database and increase the client's CPU usage. The default
value is 100ms which should work fine in most cases. The client will also adapt to situations where there are free CPUs but no more jobs and increase the interval internally and fall back to the configured
value once it got another job.

The ''-k'' flag tells the client if it should keep temporary job output files after a job is finished of if it should delete them. The default is to delete them.

\subsection{Launching}
After configuration you can simply run the client on your computation machines. On computer clusters there are often queuing systems that you have to use to gain access to the nodes.
On bwGRID for example, we could use the following short PBS (portable batch script) and submit our client to 10 nodes with 8 cores per node:
\begin{verbatim}
#!/bin/sh
#PBS -l walltime=10:00:00
#PBS -l nodes=10:ppn=8:bwgrid
cd /path/to/shared/fs/with/client/executable
./client -v0 -l -i200 -w120
\end{verbatim}
You should always run the client from within its directory (i.e. cd to the directory) to avoid problems with relative paths such as the verifier path from the example configuration above.

As soon as clients start you should be able to see jobs changing their status from ''not started'' to ''running'' in the GUI's or Web frontend's job browsers.

\subsection{Troubleshooting}
If errors or failures occur the client will always attempt to shut down cleanly, that is stop all running jobs and set their status to ''client crashed'' and write
the last lines of its log output as ''launcher output'' to each job. This can fail when network connections fail or the client receives a SIGKILL signal causing it to exit immediately.
In case of network failures you should still be able to find useful information in the client's logfile on the local filesystem.

\section{Verifiers}
Verifiers are programs that the client runs after a job finishes. Verifiers are getting passed the instance of the job and the solver output as arguments and are supposed to exit with a status code
that indicates if the solution the solver gave for the instance is correct or other result descriptions. This code will be written to the database as ''result code'' and ''verifier exit code''. Any output the verifier writes to standard out will
be written as ''verifier output''. The call specification for a verifier binary looks like this:
\begin{verbatim}
./verifier_binary <path_to_instance> <path_to_solver_output>
\end{verbatim}
We provide a verifier for the SAT problem that works on CNF instances in DIMACS format and solvers that adhere to a certain output format (see the source code). Note that you have to make sure
that your possible result codes are specified in the \textit{ResultCodes} table in the database before running clients or there will be errors when the client tries to write results. By convention,
the web frontend and GUI application consider status codes that begin with a decimal ''1'' as correct answers.

\section{Experiment priorization}
EDACC experiments can be marked as inactive and individual jobs can be prioritized. Only jobs of active experiments with priority equal to or greater than 0 are considered for processing
by the client. Futhermore, experiments can be assigned a priority. The clients will then try to match the relative number of CPUs working on an experiment with their relative priority to
all other experiments that are assigned to the same grid queue. For example, if you have three experiments with priorities 100, 200 and 300 respectively and a total number of 100 CPUs managed
by some running clients, the clients will try to have 16\% of CPUs working on the first, 33\% of CPUs working on the second and 50\% of CPUs working on the third experiment.


\end{document}
